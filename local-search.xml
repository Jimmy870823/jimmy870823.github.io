<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>應用強化學習於IMU精確動作追蹤與遠端人型機器人重定向控制</title>
    <link href="/2022/06/05/skeleton-robot/"/>
    <url>/2022/06/05/skeleton-robot/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品為碩士論文</p></blockquote><h1 id="論文摘要"><a href="#論文摘要" class="headerlink" title="論文摘要"></a>論文摘要</h1><p>設計一套基於姿態遠端控制機器人系統，操作者配戴感測器對機器人手部姿態重定向與腿部控制，搭配語音系統使人機兩端能進行語音通訊，頭部配戴VR頭戴顯示器以獲取機器人視野，藉此將自己化身為機器人與他人進行互動。</p><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><ul><li><p>操作者遠程控制系統</p><p>本研究操作者端將配戴HTC VIVE VR頭戴式顯示器以獲取機器人視野畫面與機器人頭部轉向控制，透過麥克風與喇叭與機器人端進行對話交流，人體姿態感測系統由14顆姿態感測器所組成。</p></li><li><p>人體運動捕捉系統</p><p>將人體姿態投射於Unity中，收集25個關節之三軸座標位置，並依據Kinect V2所定義之關節點作為本研究關節資料儲存形式。</p></li><li><p>機器人系統</p><p>本研究使用機器人為Nao-V6，全身配有25個自由度，主要分布於頭部、手部與足部。</p><ol><li>手部控制:  能夠透過強化學習與逆向運動學兩種模式實現。</li><li>腿部控制: 為考慮機器人安全性，腿部動作使用預設前進、側移、轉向等動作。</li><li>頭部控制: 透過虛擬實境頭盔控制機器人頭部轉向，與機器人視野追蹤。</li><li>音訊控制: 建立錄音、播音系統傳輸兩地音訊。</li></ol></li></ul><p><img src="/2022/06/05/skeleton-robot/%E7%B3%BB%E7%B5%B1%E6%9E%B6%E6%A7%8B%E5%9C%96.png" alt="系統架構圖"></p><h1 id="資料傳輸"><a href="#資料傳輸" class="headerlink" title="資料傳輸"></a>資料傳輸</h1><p>資料傳輸透過Python Socket TCP網路通訊程式實現，在傳輸系統中又分為服務端（Server）與客戶端（Client），給定IP地址、協議方式與連接埠來建立溝通。</p><p>本研究以TCP協定進行資料傳輸，由於機器人需與路由器綁定，故於此路由器建立虛擬伺服器，並設置連接埠轉發功能，客戶端與服務端集可跨域傳輸資料。</p><p><img src="/2022/06/05/skeleton-robot/TCP%E8%B3%87%E6%96%99%E5%82%B3%E8%BC%B8%E6%B5%81%E7%A8%8B.jpg" alt="TCP資料傳輸流程"></p><h1 id="系統設計"><a href="#系統設計" class="headerlink" title="系統設計"></a>系統設計</h1><p>本研究使用 <em><strong>強化學習</strong></em>  與 <em><strong>逆向運動學</strong></em>  兩方式對機器人手臂作運動重定向，分析兩種方法之優劣。</p><h2 id="手部控制"><a href="#手部控制" class="headerlink" title="手部控制"></a>手部控制</h2><h3 id="資料前處理"><a href="#資料前處理" class="headerlink" title="資料前處理"></a>資料前處理</h3><p>Unity與機器人兩座標軸不同，建立方向餘弦(DCM)矩陣，將Unity中的骨骼資訊進行座標轉換至機器人座標軸。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606135339929.png" alt="DCM matrix"></p><p><img src="/2022/06/05/skeleton-robot/%E5%9D%90%E6%A8%99%E8%BB%B8%E8%BD%89%E6%8F%9B.jpg" alt="坐標軸轉換"></p><p>座標軸轉換結果</p><p><img src="/2022/06/05/skeleton-robot/image-20220606135629470.png" alt="前處理結果"></p><h3 id="1-逆向運動學"><a href="#1-逆向運動學" class="headerlink" title="1. 逆向運動學"></a>1. 逆向運動學</h3><p>下圖為機器人左右手臂的鏈結框架。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606140055204.png" alt="D-H模型 關節軸配置"></p><p>Denavit-Hartenberg參數模型(D-H模型)，將每個連桿建構座標系，以便描述當前連桿座標系與相鄰連桿座標系之間的轉換關係，下表為Nao機器人依據鏈結框架所建置之D-H模型。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606142529516.png" alt="D-H參數表"></p><p>因左右手臂參數模型不同，故左右手臂有不同的逆向運動解。由於機器人與人體維度不同，需將前處理後人體向量映射至機器人向量，代入逆向運動學公式中，即可回推機器人手臂關節旋轉角，以驅動機器人手臂至目標位置，使機器人模仿人體的手部姿態。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606144037070.png" alt="逆向運動學解"></p><h3 id="2-強化學習"><a href="#2-強化學習" class="headerlink" title="2. 強化學習"></a>2. 強化學習</h3><p>強化學習模型使用<em><strong>行動者評論家網路</strong></em>，為一種結合策略梯度與時序差分（Temporal difference）的強化學習方法，它使用兩個深度學習模型，分別為行動者與評論家。行動者模型學習在特定環境狀態下應該執行什麼動作，當行動者模型選擇的動作被執行後，智能體與評論家模型將從環境中獲得獎勵。評論家模型的作用為優化行動者模型，將行動者模型依據前狀態採取之動作給予評級，而智能體將基於評級比較當前策略與新策略，並決定如何改進行動者模型以採取更好的行動。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/行動者評論家演算法.jpg" alt="行動者評論家演算法" style="zoom: 67%;"></div></div></div><p>給定6個目標動作，分別為歡呼、揮手、指向、擦臉、敬禮、雙手合十，這些動作是人與人之間常有的互動動作。機器人端預先設計好這6類動作，而受試者端透過人體運動捕捉系統蒐集6類運動數據，根據人體姿態能獲得所有關節之三軸位置數據。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606151145374.png" alt="6類目標動作"></p><p>行動者評論家網路以人體與機器人兩端之間的潛在向量作學習，人機兩端數據預先訓練<em><strong>變分自動編碼器網路</strong></em>（Variational Autoencoder, VAE），即可獲得兩端之間的潛在向量。</p><p><img src="/2022/06/05/skeleton-robot/skel_vae.png" alt="skel_vae"></p><p><img src="/2022/06/05/skeleton-robot/robot_vae.png" alt="robot_vae"></p><ol><li><p><em><strong>行動者-評論家網路架構</strong></em>，定義智能體之學習策略，模型前饋階段經前處理之骨骼數據會進行編碼，行動者網路將接收骨骼之隱藏含量並自動生成機器人之隱藏含量。</p></li><li><p>V-REP軟體為智能體與環境的接口，將解碼後的機器人動作於模擬器中實現，模擬器將輸出機器人關節角度與位置</p></li></ol><ul><li>位置: 計算手部向量相似度作為模型獎勵機制</li><li>角度: 角度編碼後的隱藏含量與人體隱藏含量用於模型反饋階段，透過評論家網路給予評級優化行動者網路</li></ul><ol start="3"><li>智能體將比較當前策略與新策略，改進行動者網路以採取更好的行動。</li></ol><p><img src="/2022/06/05/skeleton-robot/image-20220606152926823.png" alt="行動者評論家網路架構"></p><p>模型目標成功讓智能體學習正確策略，並讓期望的折扣獎勵最大化，模型訓練完畢後人體姿態數據將透過行動者網路生成正確之機器人隱藏含量，並對隱藏含量解碼以得到正確之關節輸出角度。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/image-20220606153331716.png" style="zoom:67%;"></div></div></div><h2 id="足部控制"><a href="#足部控制" class="headerlink" title="足部控制"></a>足部控制</h2><p>依照動畫中左大腿y軸旋轉角與右大腿z軸旋轉角來觸發足部行動。</p><ul><li>前進: 角度上升 當上升超越40度觸發</li><li>側移: 角度下降 當下降超越15度觸發</li><li>轉身: 將動畫面向方位與機器人面向方位對齊，當差距超過40度時，進行修正。</li></ul><p><img src="/2022/06/05/skeleton-robot/image-20220606213306159.png" alt="image-20220606213306159"></p><h2 id="音訊控制"><a href="#音訊控制" class="headerlink" title="音訊控制"></a>音訊控制</h2><p>語音系統由錄音系統與播放系統組成，操作者端錄音透過RMS值大小觸發，錄音系統需設定音量閥值，待RMS值大於閥值觸發錄音，由於講話換氣時RMS值會驟降，故限制RMS值小於閥值持續1秒後才中斷錄音，避免斷續錄音的問題，錄完的音檔寫入至兩端共用之雲端硬碟內。</p><p>播放系統為播放雲端硬碟中對方儲存之音檔，並持續檢查音訊檔案是否寫入完畢，寫入完畢才能進行播放動作，於兩系統加入條件讓錄音系統與播放系統能順利運行不互相衝突。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/音訊架構.jpg" alt="音訊架構"></div></div></div><p>機器人端與操作者端語音系統邏輯設計相同，機器人觸發錄音時會將全身亮起黃色LED燈，並說”start recording”，錄音結束後會將LED熄滅，並說”record over”，以提示機器人端的使用者機器人當前語音系統狀態。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/機器人端語音流程.jpg" alt="機器人端語音流程" style="zoom: 80%;"></div></div></div><h2 id="頭部控制"><a href="#頭部控制" class="headerlink" title="頭部控制"></a>頭部控制</h2><h3 id="頭部轉向"><a href="#頭部轉向" class="headerlink" title="頭部轉向"></a>頭部轉向</h3><p>將Unity與機器人座標軸對齊，並直接給定角度控制機器人頭部轉向。</p><h3 id="鏡頭輸出"><a href="#鏡頭輸出" class="headerlink" title="鏡頭輸出"></a>鏡頭輸出</h3><p>機器人鏡頭輸出有多種色彩空間與解析度供選擇，本系統將鏡頭設置為RGB色彩空間，因考慮鏡頭畫面流暢度，解析度為320*420，每秒30幀。</p><p>透過Wi-Fi連線至機器人鏡頭伺服器，電腦接收機器人視訊畫面後，取出視訊內容之像素點並進行像素點重組，設計超文字標記檔（HTML）之格式內容，將重組後之視訊內容呈現於超文字標記檔中。</p><p>視頻流可以在大多數現代瀏覽器（即Firefox、Edge、Chrome 等）中使用，輸入路由器IP與外部連接埠端口即可開啟檔案以檢視機器人視野畫面。</p><h1 id="實驗結果"><a href="#實驗結果" class="headerlink" title="實驗結果"></a>實驗結果</h1><h2 id="模型訓練"><a href="#模型訓練" class="headerlink" title="模型訓練"></a>模型訓練</h2><p>實驗中有4位受試者，共收集720筆人體運動數據，每個目標動作類別分別120筆，將每類別中9成數據用於訓練數據，1成數據用於驗證模型效能，下圖為行動者評論家網路訓練結果圖，模型訓練488個Epoch收斂，於i7-9700與NVIDIA GeForce RTX 2060訓練總時長約26.3小時。</p><img src="/2022/06/05/skeleton-robot/image-20220606220158742.png" alt="行動者評論家網路訊結果" style="zoom:50%;"><p>評論家網路透過Temporal Difference (TD) 來計算價值函數，將驗證資料隨機提取5000幀進行模型評估，下表列出不同step TD中模型評估結果，表中分別為平均獎勵與標準差，最終選擇使用1-step TD，因為它性能最佳擁有高平均獎勵與低標準差。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606220544406.png" alt="驗證不同step間模型效能"></p><h2 id="重定向效能評估"><a href="#重定向效能評估" class="headerlink" title="重定向效能評估"></a>重定向效能評估</h2><h3 id="已學習動作"><a href="#已學習動作" class="headerlink" title="已學習動作"></a>已學習動作</h3><p>針對6種學習動作歡呼(cheerup)、揮手(handwave)、指向(pfinger)、雙手合十(ppalm)、敬禮(salute)、擦臉(wface)進行效能評估，並使用關節夾角軌跡、餘弦相似度與弗雷歇距離（Fréchet distance）等方式，觀察人機之間運動軌跡的相似度。</p><p>圖中左側為<em><strong>關節夾角軌跡圖</strong></em>，右側為手部四個部位之<em><strong>餘弦相似度圖</strong></em></p><ul><li><p>夾角軌跡圖: 人體與機器人在手部姿態有相同的軌跡趨勢，雙手合十、敬禮與擦臉三動作因硬體限制在動作過程存在角度偏差值。</p></li><li><p>餘弦相似度: 6個目標動作中四個手部部位的餘弦相似度平均有0.8以上，能達到相當好的重定向效果。</p></li></ul><p><img src="/2022/06/05/skeleton-robot/image-20220606220943166.png"></p><p><img src="/2022/06/05/skeleton-robot/image-20220606221015629.png"></p><p><em><strong>弗雷歇距離</strong></em>（Fréchet distance）為一種路徑空間相似性的描述方式，基於動態規劃思想針對連續性軌跡進行匹配，將匹配距離與採樣方式合併考慮，弗雷歇距離目標求出能夠聯繫兩軌跡之間的最小距離，本研究為人體軌跡與機器人軌跡相比較。</p><p>下表為強化學習與逆向運動學兩種方法之弗雷歇距離，單位為公尺，LE、RW、LW、RW分別為左右手肘與手腕，兩種方式之弗雷歇距離大都落在4公分內。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606234125495.png" alt="learn Fréchet distance"></p><h3 id="未學習動作"><a href="#未學習動作" class="headerlink" title="未學習動作"></a>未學習動作</h3><p>未學習動作的效果能夠測試模型的泛化能力，給定6個未學習的目標動作，擊掌、喝水、戴帽子、刷牙、擁抱、擺手等，透過下表弗雷歇距離可知強化學習在執行這些未學習的動作仍然有很好的表現。</p><p><img src="/2022/06/05/skeleton-robot/image-20220606234905582.png" alt="unlearn Fréchet distance"></p><h3 id="強化學習-vs-逆向運動學"><a href="#強化學習-vs-逆向運動學" class="headerlink" title="強化學習 vs. 逆向運動學"></a>強化學習 vs. 逆向運動學</h3><p>逆向運動學以人體位置為標準依據，透過逆向運動學公式準確回推機器人關節角度以達目標位置，雖然逆向運動學方法精準，但不適用於真實應用情境中，因在做部分動作時機器人會發生自碰撞的現象。</p><p>強化學習因機器人預先編譯動作，在動作設計中避免機器人碰撞現象發生，故強化學習在匹配人體與機器人潛在向量時會找潛在向量最相近者，同時會犧牲掉部分準確度，但也能避免掉自碰撞的現象發生。</p><p>下圖中為執行雙手合十動作，第一張圖為強化學習結果，第二張圖為逆向運動學結果。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/未碰撞.jpg" alt="未碰撞" style="zoom:50%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/05/skeleton-robot/自碰撞.jpg" alt="自碰撞" style="zoom:50%;"></div></div></div><table><thead><tr><th align="center"></th><th align="center">自碰撞</th><th align="center">應用場景</th><th align="center">準確度</th></tr></thead><tbody><tr><td align="center">強化學習</td><td align="center">X</td><td align="center">實際與人交互</td><td align="center">高</td></tr><tr><td align="center">逆向運動學</td><td align="center">O</td><td align="center">機器人動作編譯</td><td align="center">較高</td></tr></tbody></table><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>參考資料1<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>碩士研究</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>人工智慧</tag>
      
      <tag>運動重定向</tag>
      
      <tag>系統設計</tag>
      
      <tag>Nao 機器人</tag>
      
      <tag>強化學習</tag>
      
      <tag>逆向運動學</tag>
      
      <tag>碩士論文</tag>
      
      <tag>虛擬實境</tag>
      
      <tag>通訊協定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>愛文芒果不良品分類競賽</title>
    <link href="/2022/06/04/ai-mango/"/>
    <url>/2022/06/04/ai-mango/</url>
    
    <content type="html"><![CDATA[<h1 id="比賽簡介"><a href="#比賽簡介" class="headerlink" title="比賽簡介"></a>比賽簡介</h1><p>台灣重要出口農產品之一的愛文芒果於近年銷量持續增長，不僅躍升為三大外銷高經濟生鮮果品之一，更將外銷國拓展至日本、中國、美國以及香港等地。</p><p>本競賽針對愛文芒果進行五類不良品分類，包括乳汁吸附、機械傷害、炭疽病、著色不佳、黑斑病等，以影像辨識快速分辨愛文芒果不良品產生原因，期許在未來以此資料進行分析及預測，提供生產者資訊，降低愛文芒果劣果率。</p><h1 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h1><p>芒果圖像透過果農拍照病標註受傷部位所收集而來，每張照片芒果可能擁有多種受傷類別，故為多分類問題。</p><ul><li><p>25768 Training set </p><p>圖像、分類標籤、受傷位置</p></li><li><p>3681 Development set</p><p>圖像、分類標籤、受傷位置</p></li><li><p>Test set</p><p>圖像</p></li></ul><h2 id="資料集圖像示意圖"><a href="#資料集圖像示意圖" class="headerlink" title="資料集圖像示意圖"></a>資料集圖像示意圖</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/04/ai-mango/38414.jpg" alt="38414" style="zoom:25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/04/ai-mango/19992.jpg" alt="19992" style="zoom:25%;"></div></div></div><h2 id="芒果受傷種類共分為五類"><a href="#芒果受傷種類共分為五類" class="headerlink" title="芒果受傷種類共分為五類"></a>芒果受傷種類共分為五類</h2><p>下表病徵圖透過原始資料集取出，一張圖像可能擁有多種病癥。</p><table><thead><tr><th align="center">不良品類別</th><th align="center">芒果圖</th></tr></thead><tbody><tr><td align="center">機械傷害</td><td align="center"><img src="/2022/06/04/ai-mango/40462_0.jpg" alt="40462_0"></td></tr><tr><td align="center">炭疽病</td><td align="center"><img src="/2022/06/04/ai-mango/00019_0.jpg" alt="00019_0" style="zoom:50%;"></td></tr><tr><td align="center">黑斑病</td><td align="center"><img src="/2022/06/04/ai-mango/47863_0.jpg" alt="47863_0" style="zoom:150%;"></td></tr><tr><td align="center">著色不佳</td><td align="center"><img src="/2022/06/04/ai-mango/00792_0.jpg" alt="00792_0" style="zoom: 50%;"></td></tr><tr><td align="center">乳汁吸附</td><td align="center"><img src="/2022/06/04/ai-mango/02015_0.jpg" alt="02015_0"></td></tr></tbody></table><h2 id="資料分析"><a href="#資料分析" class="headerlink" title="資料分析"></a>資料分析</h2><p> 圖表發現只有單一種炭疽病或著色不佳此兩種病癥佔多數，其餘為多種病癥混合，或其他三種症狀，資料嚴重不平衡。  </p><p><img src="/2022/06/04/ai-mango/image-20220605152135404.png" alt="資料分析表"></p><h2 id="資料增強-Data-Augment"><a href="#資料增強-Data-Augment" class="headerlink" title="資料增強(Data Augment)"></a>資料增強(Data Augment)</h2><p>訓練資料集的25768張圖片中，有87%(22522張圖片)只有一種病徵，故於訓練階段時用這22522張圖片做訓練，期望可以更準確的抓取病徵特徵，而不會產生病徵之間的混淆。</p><p>針對機械傷害、黑斑病、乳汁吸附三類圖像進行資料擴增，進行圖片翻轉、平移、明暗度、比例尺等方式將此三種類別資料擴增。</p><h1 id="模型訓練"><a href="#模型訓練" class="headerlink" title="模型訓練"></a>模型訓練</h1><p>模型架構為EfficientNetB0 + Dence(128) + 五個輸出層(Dence(1)) ，並且不使用pretrain_weight，讓模型重新訓練，optimizer使用Adam，模型中加入sample_weight參數，針對有著色不佳的圖片sample_weight設為5，其他病徵則設為1，將模型設定為多輸出模型(對輸入的圖像作五個binary的預測)。</p><p>模型最後的輸出為五個值，所以設定每個類別各自的閥值。在本次競賽中我們將五個類別閥值皆設為0.15。若閥值設定太高，會有一張圖片五個預測值都為0的情況。<img src="/2022/06/04/ai-mango/image-20220605172834946.png" alt="網路架構"></p><h1 id="評估公式"><a href="#評估公式" class="headerlink" title="評估公式"></a>評估公式</h1><p><img src="/2022/06/04/ai-mango/image-20220605220303830.png" alt="評估標準"></p><h1 id="競賽成果"><a href="#競賽成果" class="headerlink" title="競賽成果"></a>競賽成果</h1><p>將預測結果存成特定csv格式，上傳結果即可獲得排名與分數。</p><ul><li>Public 為競賽期間測試Development set最佳成果</li><li>Private為測試Test set成果</li></ul><table><thead><tr><th>Leaderboard</th><th>Rank</th><th>Score</th></tr></thead><tbody><tr><td>Public</td><td>37&#x2F;222</td><td>0.6573533</td></tr><tr><td>Private</td><td>40&#x2F;222</td><td>0.6628388</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI CUP 2020</tag>
      
      <tag>影像處理</tag>
      
      <tag>人工智慧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>結合虛擬實境與慣性姿態感測器之防疫機器代理人</title>
    <link href="/2022/06/03/future-tech/"/>
    <url>/2022/06/03/future-tech/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品<strong>2021未來科技獎競賽獲獎</strong></p></blockquote><h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><p>使用者將會透過VR眼鏡看到機器人360環景影像，接著使用者姿態動作改變時，其姿態動作感測訊號將透過WIFI送至雲端進行深度學習模型辨識。當模型辨識到使用者之動作時，將驅動機器人進行相同的動作。</p><!-- more --><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><ol><li>受試者穿戴VR裝置，並安裝360鏡頭於機器人身上，使受試者與機器人有相同的視野，能夠顯得更加臨場感。</li><li>姿態感測器組成一套人體姿態量測系統，利用演算法進行角度融合運算，透過Wi-Fi將資料傳輸到電腦端進行所有感測器的整合與姿態計算。</li><li>以CNN+LSTM模型訓練九軸資料，並於測試端進行即時動作識別，再將所判斷的動作指令透過藍芽傳輸與機器人作溝通，命令機器人作出相對應的動作。</li></ol><p><img src="/2022/06/03/future-tech/image-20220603234742622.png" alt="系統架構圖"></p><h2 id="使用設備"><a href="#使用設備" class="headerlink" title="使用設備"></a>使用設備</h2><h3 id="設備總覽"><a href="#設備總覽" class="headerlink" title="設備總覽"></a>設備總覽</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/image-20220604031854938.png" alt="使用設備" style="zoom: 50%;"></div></div></div><h3 id="慣性感測器模組"><a href="#慣性感測器模組" class="headerlink" title="慣性感測器模組"></a>慣性感測器模組</h3><p>自行開發的慣性感測器，慣性感測單元具有九軸加速規的高感度設計，內含三軸陀螺儀、加速度計及磁力計，角度探測精度可以小於0.1度。</p><p>Wifi傳輸模組，可以輕鬆地將串列設備連接至網路，透過UART與微控制器溝通。微處理器用來控制周遭的電路。</p><img src="/2022/06/03/future-tech/image-20220604032002448.png" alt="慣性感測器硬體系統" style="zoom: 67%;"><h3 id="VR-amp-360相機"><a href="#VR-amp-360相機" class="headerlink" title="VR&amp;360相機"></a>VR&amp;360相機</h3><p>360相機為theta v，擁有360度全景直播功能，透過直播網路串流將影像投射到Unity球體上，再將VR視角置於球體正中心，戴上VR眼鏡後，即可如身歷其境般看到機器人的360視角</p><img src="/2022/06/03/future-tech/image-20220604033232448.png" alt="VR&360相機" style="zoom:67%;"><h2 id="模型架構"><a href="#模型架構" class="headerlink" title="模型架構"></a>模型架構</h2><p>因考慮到機器人控制之即時性，姿態感測訊號訓練資料時間範圍為動作開始時間點後0.1秒，訓練Deep Conv-LSTM 模型，進行動作姿態辨識。</p><p>受試者皆可以有效完成六種指定之動作，包括左手抬起、右手抬起、雙手抬起、前進、左轉以及右轉之動作，並且可在各種環境下皆可進行使用，不易受到外界雜訊干擾。</p><img src="/2022/06/03/future-tech/image-20220604030119199.png" alt="AI模型架構" style="zoom: 50%;"><h2 id="動作識別"><a href="#動作識別" class="headerlink" title="動作識別"></a>動作識別</h2><p>使用者執行動作時，姿態慣性感測訊號送至深度學習模型進行判別，並根據其判別結果，送出動作指令給機器人，使機器人做出相對應之動作。</p><p><img src="/2022/06/03/future-tech/image-20220604030503219.png"></p><h1 id="實體展示"><a href="#實體展示" class="headerlink" title="實體展示"></a>實體展示</h1><p>於台北世貿實體展示，並開放觀眾參觀與詢問問題。</p><p>向高中學生展示與介紹我們的系統，開放參觀者親自體驗我們的系統</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7708.png" alt="IMG_7708" style="zoom: 25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7704.png" alt="IMG_7704" style="zoom: 25%;"></div></div></div><p><strong>此作品榮獲未來科技獎</strong></p><p><img src="/2022/06/03/future-tech/IMG_7743.png"></p><p><img src="/2022/06/03/future-tech/IMG_7802.png"></p><blockquote><p>完整競賽影片 : <a href="https://youtu.be/XFN2XDS2GfY"><em>未來科技獎</em></a> (於影片後段有demo影片)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>機器人</tag>
      
      <tag>2021未來科技獎</tag>
      
      <tag>人工智慧</tag>
      
      <tag>系統設計</tag>
      
      <tag>虛擬實境</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>運動捕捉系統</title>
    <link href="/2022/06/02/imu_skeleton/"/>
    <url>/2022/06/02/imu_skeleton/</url>
    
    <content type="html"><![CDATA[<h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><blockquote><p>碩士研究 Project</p></blockquote><p>​配置感測器於操作者之胸部、臀部、左右手之手腕、手軸、手臂、肩膀、左右腳之大腿與小腿，而感測器配戴方式為將每個感測器之燈號方向朝向軀幹，以軀幹為中心所配置。配戴感測器後，同時偵測全身感測器之四元數，透過藍芽傳輸四元數數據至電腦端，並將IMU依據關節分段儲存（IMU to segment, I2S），進行I2S分配與I2S矯正後即可透過四元數控制相對應之關節點旋轉，於Unity遊戲引擎內呈現出人體姿態。</p><!-- more --><h1 id="系統特色"><a href="#系統特色" class="headerlink" title="系統特色"></a>系統特色</h1><ul><li><p>Record mode</p><p>​存取動畫骨骼中25個關節之三軸座標位置與對應時間點，透過錄製鍵與存取鍵將人體姿態依據csv格式做儲存。(於程式中可再加入其他部位的關節資訊與關節旋轉角)</p></li><li><p>Real-time mode</p><p>​將動畫中25個關節位置資訊實時存取，並以txt格式存取並實時更新關節旋轉角與位置資訊。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/關節位置.png" style="zoom: 80%;"></div></div></div></li></ul><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><p><img src="/2022/06/02/imu_skeleton/%E9%81%8B%E5%8B%95%E6%8D%95%E6%8D%89%E7%B3%BB%E7%B5%B1.png"></p><h2 id="I2S-矯正"><a href="#I2S-矯正" class="headerlink" title="I2S 矯正"></a>I2S 矯正</h2><p>慣性感測器(IMU)與動畫人體模型各自擁有專屬的座標系，圖中紅色為X軸、藍色為Y軸、綠色為Z軸，IMU穿戴於人體身上後無法與Unity座標系相符合，需透過四元數運算將全身慣性感測器座標系轉換至Unity座標系，進行I2S矯正並紀錄正確四元數數值，以達成人體姿態重定向目標。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/坐標系.png" style="zoom: 67%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/%E7%9F%AF%E6%AD%A3.png"></div></div></div><p>​使用者穿戴慣性感測器時將以T-pose進行姿態矯正，下面影片中j動畫呈現奇怪姿態，因IMU座標軸尚未與Unity座標軸對齊，操作者須保持T-pose，按下calibrate鍵，即矯正完畢，可呈現出人體正確姿態。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/image-20220602180733685.png" alt="image-20220602180733685" style="zoom: 25%;"></div></div></div><p><img src="/2022/06/02/imu_skeleton/calibrate.gif"></p><h1 id="系統功能呈現"><a href="#系統功能呈現" class="headerlink" title="系統功能呈現"></a>系統功能呈現</h1><h2 id="運動捕捉系統"><a href="#運動捕捉系統" class="headerlink" title="運動捕捉系統"></a>運動捕捉系統</h2><p>運動捕捉系統即時動畫呈現</p><p><img src="/2022/06/02/imu_skeleton/forward.gif"></p><p><img src="/2022/06/02/imu_skeleton/rightward.gif"></p><p><img src="/2022/06/02/imu_skeleton/backward.gif"></p><p><img src="/2022/06/02/imu_skeleton/leftward_and_sit.gif"></p><blockquote><p>完整demo 影片: <a href="https://www.youtube.com/watch?v=ozX-XhBwfu8"><em>IMU in Unity Skeleton</em></a></p></blockquote><h2 id="可示化人體骨骼"><a href="#可示化人體骨骼" class="headerlink" title="可示化人體骨骼"></a>可示化人體骨骼</h2><p>錄製關節角度後於python端呈現人體骨骼姿態</p><p><img src="/2022/06/02/imu_skeleton/handwave2.gif" alt="skeleton"></p>]]></content>
    
    
    <categories>
      
      <category>碩士研究</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>運動捕捉</tag>
      
      <tag>運動重定向</tag>
      
      <tag>人機介面</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
