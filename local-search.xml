<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>使用強化學習之動作同步遠端操控人型機器人</title>
    <link href="/2022/08/08/skeleton-robot/"/>
    <url>/2022/08/08/skeleton-robot/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品為碩士論文，並參與2022年未來科技獎競賽</p><p>完整競賽影片 : <a href="https://youtu.be/gdf7ZX6rv9M"><em>2022未來科技獎</em></a> (於影片後段有demo影片)</p></blockquote><h1 id="論文摘要"><a href="#論文摘要" class="headerlink" title="論文摘要"></a>論文摘要</h1><p>設計一套基於姿態遠端控制機器人系統，操作者配戴感測器對機器人手部姿態重定向與腿部控制，搭配語音系統使人機兩端能進行語音通訊，頭部配戴VR頭戴顯示器以獲取機器人視野，藉此將自己化身為機器人與他人進行互動。</p><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><ul><li><p>操作者遠程控制系統</p><p>本研究操作者端將配戴HTC VIVE VR頭戴式顯示器以獲取機器人視野畫面與機器人頭部轉向控制，透過麥克風與喇叭與機器人端進行對話交流，人體姿態感測系統由14顆姿態感測器所組成。</p></li><li><p>人體運動捕捉系統</p><p>將人體姿態投射於Unity中，收集25個關節之三軸座標位置，並依據Kinect V2所定義之關節點作為本研究關節資料儲存形式。</p></li><li><p>機器人系統</p><p>本研究使用機器人為Nao-V6，全身配有25個自由度，主要分布於頭部、手部與足部。</p><ol><li>手部控制:  能夠透過強化學習與逆向運動學兩種模式實現。</li><li>腿部控制: 為考慮機器人安全性，腿部動作使用預設前進、側移、轉向等動作。</li><li>頭部控制: 透過虛擬實境頭盔控制機器人頭部轉向，與機器人視野追蹤。</li><li>音訊控制: 建立錄音、播音系統傳輸兩地音訊。</li></ol></li></ul><p><img src="/2022/08/08/skeleton-robot/%E7%B3%BB%E7%B5%B1%E6%9E%B6%E6%A7%8B%E5%9C%96.png" alt="系統架構圖"></p><h1 id="資料傳輸"><a href="#資料傳輸" class="headerlink" title="資料傳輸"></a>資料傳輸</h1><p>資料傳輸透過Python Socket TCP網路通訊程式實現，在傳輸系統中又分為服務端（Server）與客戶端（Client），給定IP地址、協議方式與連接埠來建立溝通。</p><p>本研究以TCP協定進行資料傳輸，由於機器人需與路由器綁定，故於此路由器建立虛擬伺服器，並設置連接埠轉發功能，客戶端與服務端集可跨域傳輸資料。</p><p><img src="/2022/08/08/skeleton-robot/TCP%E8%B3%87%E6%96%99%E5%82%B3%E8%BC%B8%E6%B5%81%E7%A8%8B.jpg" alt="TCP資料傳輸流程"></p><h1 id="系統設計"><a href="#系統設計" class="headerlink" title="系統設計"></a>系統設計</h1><p>本研究使用 <em><strong>強化學習</strong></em>  與 <em><strong>逆向運動學</strong></em>  兩方式對機器人手臂作運動重定向，分析兩種方法之優劣。</p><h2 id="手部控制"><a href="#手部控制" class="headerlink" title="手部控制"></a>手部控制</h2><h3 id="資料前處理"><a href="#資料前處理" class="headerlink" title="資料前處理"></a>資料前處理</h3><p>Unity與機器人兩座標軸不同，建立方向餘弦(DCM)矩陣，將Unity中的骨骼資訊進行座標轉換至機器人座標軸。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606135339929.png" alt="DCM matrix"></p><p><img src="/2022/08/08/skeleton-robot/%E5%9D%90%E6%A8%99%E8%BB%B8%E8%BD%89%E6%8F%9B.jpg" alt="坐標軸轉換"></p><p>座標軸轉換結果</p><p><img src="/2022/08/08/skeleton-robot/image-20220606135629470.png" alt="前處理結果"></p><h3 id="1-逆向運動學"><a href="#1-逆向運動學" class="headerlink" title="1. 逆向運動學"></a>1. 逆向運動學</h3><p>下圖為機器人左右手臂的鏈結框架。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606140055204.png" alt="D-H模型 關節軸配置"></p><p>Denavit-Hartenberg參數模型(D-H模型)，將每個連桿建構座標系，以便描述當前連桿座標系與相鄰連桿座標系之間的轉換關係，下表為Nao機器人依據鏈結框架所建置之D-H模型。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606142529516.png" alt="D-H參數表"></p><p>因左右手臂參數模型不同，故左右手臂有不同的逆向運動解。由於機器人與人體維度不同，需將前處理後人體向量映射至機器人向量，代入逆向運動學公式中，即可回推機器人手臂關節旋轉角，以驅動機器人手臂至目標位置，使機器人模仿人體的手部姿態。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606144037070.png" alt="逆向運動學解"></p><h3 id="2-強化學習"><a href="#2-強化學習" class="headerlink" title="2. 強化學習"></a>2. 強化學習</h3><p>強化學習模型使用<em><strong>行動者評論家網路</strong></em>，為一種結合策略梯度與時序差分（Temporal difference）的強化學習方法，它使用兩個深度學習模型，分別為行動者與評論家。行動者模型學習在特定環境狀態下應該執行什麼動作，當行動者模型選擇的動作被執行後，智能體與評論家模型將從環境中獲得獎勵。評論家模型的作用為優化行動者模型，將行動者模型依據前狀態採取之動作給予評級，而智能體將基於評級比較當前策略與新策略，並決定如何改進行動者模型以採取更好的行動。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/行動者評論家演算法.jpg" alt="行動者評論家演算法" style="zoom: 67%;"></div></div></div><p>給定6個目標動作，分別為歡呼、揮手、指向、擦臉、敬禮、雙手合十，這些動作是人與人之間常有的互動動作。機器人端預先設計好這6類動作，而受試者端透過人體運動捕捉系統蒐集6類運動數據，根據人體姿態能獲得所有關節之三軸位置數據。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606151145374.png" alt="6類目標動作"></p><p>行動者評論家網路以人體與機器人兩端之間的潛在向量作學習，人機兩端數據預先訓練<em><strong>變分自動編碼器網路</strong></em>（Variational Autoencoder, VAE），即可獲得兩端之間的潛在向量。</p><p><img src="/2022/08/08/skeleton-robot/skel_vae.png" alt="skel_vae"></p><p><img src="/2022/08/08/skeleton-robot/robot_vae.png" alt="robot_vae"></p><ol><li><p><em><strong>行動者-評論家網路架構</strong></em>，定義智能體之學習策略，模型前饋階段經前處理之骨骼數據會進行編碼，行動者網路將接收骨骼之隱藏含量並自動生成機器人之隱藏含量。</p></li><li><p>V-REP軟體為智能體與環境的接口，將解碼後的機器人動作於模擬器中實現，模擬器將輸出機器人關節角度與位置</p></li></ol><ul><li>位置: 計算手部向量相似度作為模型獎勵機制</li><li>角度: 角度編碼後的隱藏含量與人體隱藏含量用於模型反饋階段，透過評論家網路給予評級優化行動者網路</li></ul><ol start="3"><li>智能體將比較當前策略與新策略，改進行動者網路以採取更好的行動。</li></ol><p><img src="/2022/08/08/skeleton-robot/image-20220606152926823.png" alt="行動者評論家網路架構"></p><p>模型目標成功讓智能體學習正確策略，並讓期望的折扣獎勵最大化，模型訓練完畢後人體姿態數據將透過行動者網路生成正確之機器人隱藏含量，並對隱藏含量解碼以得到正確之關節輸出角度。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/image-20220606153331716.png" style="zoom:67%;"></div></div></div><h2 id="足部控制"><a href="#足部控制" class="headerlink" title="足部控制"></a>足部控制</h2><p>依照動畫中左大腿y軸旋轉角與右大腿z軸旋轉角來觸發足部行動。</p><ul><li>前進: 角度上升 當上升超越40度觸發</li><li>側移: 角度下降 當下降超越15度觸發</li><li>轉身: 將動畫面向方位與機器人面向方位對齊，當差距超過40度時，進行修正。</li></ul><p><img src="/2022/08/08/skeleton-robot/image-20220606213306159.png" alt="image-20220606213306159"></p><h2 id="音訊控制"><a href="#音訊控制" class="headerlink" title="音訊控制"></a>音訊控制</h2><p>語音系統由錄音系統與播放系統組成，操作者端錄音透過RMS值大小觸發，錄音系統需設定音量閥值，待RMS值大於閥值觸發錄音，由於講話換氣時RMS值會驟降，故限制RMS值小於閥值持續1秒後才中斷錄音，避免斷續錄音的問題，錄完的音檔寫入至兩端共用之雲端硬碟內。</p><p>播放系統為播放雲端硬碟中對方儲存之音檔，並持續檢查音訊檔案是否寫入完畢，寫入完畢才能進行播放動作，於兩系統加入條件讓錄音系統與播放系統能順利運行不互相衝突。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/音訊架構.jpg" alt="音訊架構"></div></div></div><p>機器人端與操作者端語音系統邏輯設計相同，機器人觸發錄音時會將全身亮起黃色LED燈，並說”start recording”，錄音結束後會將LED熄滅，並說”record over”，以提示機器人端的使用者機器人當前語音系統狀態。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/機器人端語音流程.jpg" alt="機器人端語音流程" style="zoom: 80%;"></div></div></div><h2 id="頭部控制"><a href="#頭部控制" class="headerlink" title="頭部控制"></a>頭部控制</h2><h3 id="頭部轉向"><a href="#頭部轉向" class="headerlink" title="頭部轉向"></a>頭部轉向</h3><p>將Unity與機器人座標軸對齊，並直接給定角度控制機器人頭部轉向。</p><h3 id="鏡頭輸出"><a href="#鏡頭輸出" class="headerlink" title="鏡頭輸出"></a>鏡頭輸出</h3><p>機器人鏡頭輸出有多種色彩空間與解析度供選擇，本系統將鏡頭設置為RGB色彩空間，因考慮鏡頭畫面流暢度，解析度為320*420，每秒30幀。</p><p>透過Wi-Fi連線至機器人鏡頭伺服器，電腦接收機器人視訊畫面後，取出視訊內容之像素點並進行像素點重組，設計超文字標記檔（HTML）之格式內容，將重組後之視訊內容呈現於超文字標記檔中。</p><p>視頻流可以在大多數現代瀏覽器（即Firefox、Edge、Chrome 等）中使用，輸入路由器IP與外部連接埠端口即可開啟檔案以檢視機器人視野畫面。</p><h1 id="實驗結果"><a href="#實驗結果" class="headerlink" title="實驗結果"></a>實驗結果</h1><h2 id="模型訓練"><a href="#模型訓練" class="headerlink" title="模型訓練"></a>模型訓練</h2><p>實驗中有4位受試者，共收集720筆人體運動數據，每個目標動作類別分別120筆，將每類別中9成數據用於訓練數據，1成數據用於驗證模型效能，下圖為行動者評論家網路訓練結果圖，模型訓練488個Epoch收斂，於i7-9700與NVIDIA GeForce RTX 2060訓練總時長約26.3小時。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/ablation study.png" alt="行動者評論家網路訓練結果" style="zoom:50%;"></div></div></div><p>評論家網路透過Temporal Difference (TD) 來計算價值函數，將驗證資料隨機提取5000幀進行模型評估，下表列出不同step TD中模型評估結果，表中分別為平均獎勵與標準差，最終選擇使用1-step TD，因為它性能最佳擁有高平均獎勵與低標準差。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606220544406.png" alt="驗證不同step間模型效能"></p><h2 id="重定向效能評估"><a href="#重定向效能評估" class="headerlink" title="重定向效能評估"></a>重定向效能評估</h2><h3 id="已學習動作"><a href="#已學習動作" class="headerlink" title="已學習動作"></a>已學習動作</h3><p>針對6種學習動作歡呼(cheerup)、揮手(handwave)、指向(pfinger)、雙手合十(ppalm)、敬禮(salute)、擦臉(wface)進行效能評估，並使用關節夾角軌跡、餘弦相似度與弗雷歇距離（Fréchet distance）等方式，觀察人機之間運動軌跡的相似度。</p><p>圖中左側為<em><strong>關節夾角軌跡圖</strong></em>，右側為手部四個部位之<em><strong>餘弦相似度圖</strong></em></p><ul><li><p>夾角軌跡圖: 人體與機器人在手部姿態有相同的軌跡趨勢，雙手合十、敬禮與擦臉三動作因硬體限制在動作過程存在角度偏差值。</p></li><li><p>餘弦相似度: 6個目標動作中四個手部部位的餘弦相似度平均有0.8以上，能達到相當好的重定向效果。</p></li></ul><p><img src="/2022/08/08/skeleton-robot/image-20220606220943166.png"></p><p><img src="/2022/08/08/skeleton-robot/image-20220606221015629.png"></p><p><em><strong>弗雷歇距離</strong></em>（Fréchet distance）為一種路徑空間相似性的描述方式，基於動態規劃思想針對連續性軌跡進行匹配，將匹配距離與採樣方式合併考慮，弗雷歇距離目標求出能夠聯繫兩軌跡之間的最小距離，本研究為人體軌跡與機器人軌跡相比較。</p><p>下表為強化學習與逆向運動學兩種方法之弗雷歇距離，單位為公尺，LE、RW、LW、RW分別為左右手肘與手腕，兩種方式之弗雷歇距離大都落在4公分內。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606234125495.png" alt="learn Fréchet distance"></p><h3 id="未學習動作"><a href="#未學習動作" class="headerlink" title="未學習動作"></a>未學習動作</h3><p>未學習動作的效果能夠測試模型的泛化能力，給定6個未學習的目標動作，擊掌、喝水、戴帽子、刷牙、擁抱、擺手等，透過下表弗雷歇距離可知強化學習在執行這些未學習的動作仍然有很好的表現。</p><p><img src="/2022/08/08/skeleton-robot/image-20220606234905582.png" alt="unlearn Fréchet distance"></p><h3 id="強化學習-vs-逆向運動學"><a href="#強化學習-vs-逆向運動學" class="headerlink" title="強化學習 vs. 逆向運動學"></a>強化學習 vs. 逆向運動學</h3><p><strong>逆向運動學</strong>以人體位置為標準依據，透過逆向運動學公式解準確回推機器人關節角度以達到目標位置，然而人與機器人手臂配置不同，因此在做部分動作時機器人會發生自碰撞的現象如下圖紅色區塊處，然而在執行擁抱動作時黃色區塊位置能看出逆向運動學法於手腕處並未呈現出擁抱正確的姿態，故逆向運動學法不適用於真實應用情境中。</p><p><strong>強化式學習</strong>對於機器人預先編譯動作，資料增強階段加入限定範圍雜訊避免機器人自碰撞現象發生，因此在匹配人體與機器人潛在向量時能夠避免自碰撞現象發生，且能夠根據軌跡經驗做出正確的旋轉角，雖然考慮自碰撞問題會犧牲掉部分準確度，但在容許誤差範圍內此方法適用於與人互動應用情境中。</p><p>實際應用場域中，數據經過Wi-Fi遠端傳輸，可能會有網路卡頓與封包掉點等情況發生，逆向運動學僅能依據當下位置推算機器人行為，未能考慮時序資訊，強化式學習法因學習序列的決策，會考慮時序上動作機率的問題，依據軌跡經驗推測行為，在封包掉點的情境中強化式學習法亦為可行方案。</p><blockquote><p> 目標學習動作-雙手合十 (自碰撞</p></blockquote><p><img src="/2022/08/08/skeleton-robot/%E8%87%AA%E7%A2%B0%E6%92%9E.png" alt="自碰撞"></p><blockquote><p>未學習動作-擁抱 (關節旋轉不正確</p></blockquote><p><img src="/2022/08/08/skeleton-robot/%E6%97%8B%E8%BD%89%E4%B8%8D%E6%AD%A3%E7%A2%BA.png" alt="旋轉不正確"></p><blockquote><p>方法分析</p></blockquote><table><thead><tr><th align="center"></th><th align="center">自碰撞考慮</th><th align="center">關節旋轉考慮</th><th align="center">時序資訊</th><th align="center">應用場景</th><th align="center">準確度</th></tr></thead><tbody><tr><td align="center">強化學習</td><td align="center">O</td><td align="center">O</td><td align="center">O</td><td align="center">實際與人交互</td><td align="center">高</td></tr><tr><td align="center">逆向運動學</td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center">機器人快速動作編譯</td><td align="center">較高</td></tr></tbody></table><h3 id="系統評估"><a href="#系統評估" class="headerlink" title="系統評估"></a>系統評估</h3><p>要求一位未曾參與過實驗的受試者根據自身習慣執行6個學習的目標動作，其中每個動作做20次，依據強化式學習的方法對機器人手部做運動重定向，對每個動作的20次數據取平均弗雷歇距離，測試手部系統重定向的穩定度，本系統平均軌跡誤差為2.8公分，在重定向控制上具備很高的穩定度。</p><p><img src="/2022/08/08/skeleton-robot/image-20220619221843377.png" alt="系統評估"></p><h3 id="系統整合應用"><a href="#系統整合應用" class="headerlink" title="系統整合應用"></a>系統整合應用</h3><p>設計一應用場景以結合各個系統的控制，場景內容操作者於甲地穿戴慣性感測器、虛擬實境頭戴式裝置與虛擬實境把手，遠程控制乙地之機器人，透過自身姿態控制機器人與機器人端的用戶打招呼，並向機器人端之用戶A拿取餅乾給用戶B，以達成互動的目標。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/1.png" alt="控制機器人行走"></div><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/2.png" alt="控制機器人轉身面對持有餅乾的用戶A"></div><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/3.png" alt="向用戶A拿取餅乾"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/4.png" alt="與用戶A招呼以示謝意"></div><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/5.png" alt="控制機器人轉身面對並走向用戶B"></div><div class="group-image-wrap"><img src="/2022/08/08/skeleton-robot/6.png" alt="將餅乾傳遞給用戶B"></div></div></div>]]></content>
    
    
    <categories>
      
      <category>碩士研究</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>人工智慧</tag>
      
      <tag>運動重定向</tag>
      
      <tag>系統設計</tag>
      
      <tag>Nao 機器人</tag>
      
      <tag>強化學習</tag>
      
      <tag>逆向運動學</tag>
      
      <tag>碩士論文</tag>
      
      <tag>虛擬實境</tag>
      
      <tag>通訊協定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle競賽銀牌-APTOS 2019 Blindness Detection</title>
    <link href="/2022/08/08/blindness-detect/"/>
    <url>/2022/08/08/blindness-detect/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品為大學專題並參與Kaggle大數據競賽榮獲銀獎</p><p>此作品榮獲2020元智大學人工智慧競賽技術與應用組優等</p></blockquote><h1 id="比賽簡介"><a href="#比賽簡介" class="headerlink" title="比賽簡介"></a>比賽簡介</h1><p>糖尿病視網膜病變，此疾病為視網膜血管因病變導致血管的滲漏，造成視網膜組織傷害，以致所看到的影像變成模糊、扭曲或部份喪失。此疾病是導致成年人失明的三大主因之一。</p><p>這項競賽由APTOS : Asia Pacific Tele-Ophthalmology Society所舉辨，APTOS為亞太地區一群傑出的遠程眼科專家於2016年5月創立，匯集臨床醫師、研究人員、技術人員相互交流合作，促進眼科學的進展。APTOS欲透過人工智慧的力量，幫助醫療資源貧乏地區的人民預防此種疾病。<img src="/2022/08/08/blindness-detect/header.png" alt="header"></p><h1 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h1><h2 id="比賽資料集-APTOS-2019-Blindness-Detection"><a href="#比賽資料集-APTOS-2019-Blindness-Detection" class="headerlink" title="比賽資料集 - APTOS 2019 Blindness Detection"></a>比賽資料集 - APTOS 2019 Blindness Detection</h2><p>這些圖像從多個診所使用各種相機收集而來。圖像可能包含偽影、失焦、曝光不足或過度曝光。</p><ul><li>3662 Training set (有標籤)</li><li>1928 Public test set (無標籤，用於比賽期間的Public Leaderboard)</li><li>20GB Private test set (隱藏的資料，用於結算比賽成績的Private Leaderboard)</li></ul><p>本此比賽主辦方所提供的資料庫根據以下等級，臨床醫生根據0至4的等級評估每張圖像中糖尿病視網膜病變：</p><table><thead><tr><th align="center">病變程度類別</th><th align="center">眼底鏡照</th><th>症狀</th></tr></thead><tbody><tr><td align="center">0-NO DR</td><td align="center"><img src="/2022/08/08/blindness-detect/0.jpg" alt="0" style="zoom: 67%;"></td><td>健康的眼睛</td></tr><tr><td align="center">1-Mild DR</td><td align="center"><img src="/2022/08/08/blindness-detect/1.jpg" alt="1" style="zoom: 67%;"></td><td>僅限微動脈瘤</td></tr><tr><td align="center">2-Moderate DR</td><td align="center"><img src="/2022/08/08/blindness-detect/2.jpg" alt="2" style="zoom: 67%;"></td><td>微動脈瘤視網膜內出血</td></tr><tr><td align="center">3-Severe DR</td><td align="center"><img src="/2022/08/08/blindness-detect/3.jpg" alt="3" style="zoom: 67%;"></td><td>大量視網膜內出血念珠菌感染視網膜內微血管異常增長但沒有PDR現象</td></tr><tr><td align="center">4-Proliferative DR</td><td align="center"><img src="/2022/08/08/blindness-detect/4.png" alt="4" style="zoom: 67%;"></td><td>以下兩種情況之一：新生血管形成玻璃體&#x2F;視網膜前出血</td></tr></tbody></table><h2 id="擴增資料集-2015-Diabetic-Retinopathy-Detection"><a href="#擴增資料集-2015-Diabetic-Retinopathy-Detection" class="headerlink" title="擴增資料集-2015 Diabetic Retinopathy Detection"></a>擴增資料集-2015 Diabetic Retinopathy Detection</h2><p>2015在Kaggle由California Healthcare Foundation贊助舉辦的另一場糖尿病視網膜病變檢測競賽所提供的資料庫，擁有與本次競賽相同的分類標籤與眼底鏡圖片。可作為本次競賽的擴充資料使用。</p><ul><li>35126 Training set (有標籤)</li><li>53576 Training set (有標籤)</li></ul><h1 id="資料分析"><a href="#資料分析" class="headerlink" title="資料分析"></a>資料分析</h1><p>本次比賽的訓練數據集非常不平衡。沒有DR的圖像相較於具有嚴重DR條件的圖像多十倍。因此我們需要對此資料集進行擴充。</p><p><img src="/2022/08/08/blindness-detect/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90.png" alt="資料分析"></p><h1 id="訓練前處理"><a href="#訓練前處理" class="headerlink" title="訓練前處理"></a>訓練前處理</h1><h2 id="資料增強"><a href="#資料增強" class="headerlink" title="資料增強"></a>資料增強</h2><p>為確保我們的網路在訓練資料時不會產生過度擬合(Over-fitting)的現象，我們使用了資料增強(Data augmentation)技術，實作方式即是每張圖像傳入神經網路訓練前，對其進行隨機的旋轉、調整大小、比例尺寸，或者改變亮度色溫、翻轉等處理。如此操作可以提升資料多樣性，避免網路過度擬合，且增強網路的泛化能力。</p><p><img src="/2022/08/08/blindness-detect/%E5%89%8D%E8%99%95%E7%90%86.png" alt="前處理"> </p><h2 id="遷移式學習"><a href="#遷移式學習" class="headerlink" title="遷移式學習"></a>遷移式學習</h2><p>這次競賽我們選用了來自Google的EfficientNet ，利用ImageNet的pretrain weight預先對2015和2019年的資料集做遷移式學習，再用訓練完的weight對2019年的資料做fine-tune，以保證模型收斂能確實符合本次競賽。</p><blockquote><p>基於EfficientNet使用遷移式學習所得到準確度</p></blockquote><table><thead><tr><th align="center">Model</th><th align="center">bs&#x2F;ep&#x2F;sz</th><th align="center">CV</th><th align="center">LB</th></tr></thead><tbody><tr><td align="center"><strong>EfficientNet b4</strong></td><td align="center">64&#x2F;10&#x2F;224</td><td align="center">0.9381</td><td align="center">0.813</td></tr><tr><td align="center"><strong>EfficientNet b5</strong></td><td align="center">32&#x2F;10&#x2F;260</td><td align="center">0.9454</td><td align="center">0.817</td></tr></tbody></table><p>(bs: batch size, ep: epoch, sz: input size, CV: cross validation, LB: Leader Board)</p><h2 id="K-fold"><a href="#K-fold" class="headerlink" title="K-fold"></a>K-fold</h2><p>K-fold是常用的交叉驗證(Cross validation)方法。如Fig. 4-3，做法是將資料隨機平均分成5個集合，然後將其中一個集合當做驗證資料，剩下的4個集合做為訓練資料，如此重複進行直到每一個集合都被當做驗證資料為止。以訓練完的這五個模型對資料進行預測，最後將這五次預測結果取平均來作為最終的預測答案。</p><h2 id="前處理訓練流程圖"><a href="#前處理訓練流程圖" class="headerlink" title="前處理訓練流程圖"></a>前處理訓練流程圖</h2><p><img src="/2022/08/08/blindness-detect/%E5%89%8D%E8%99%95%E7%90%86%E8%88%87%E8%A8%93%E7%B7%B4%E6%B5%81%E7%A8%8B%E5%9C%96.png" alt="前處理與訓練流程圖"></p><h1 id="訓練後處理"><a href="#訓練後處理" class="headerlink" title="訓練後處理"></a>訓練後處理</h1><h2 id="TTA-Test-Time-Augmentation"><a href="#TTA-Test-Time-Augmentation" class="headerlink" title="TTA(Test Time Augmentation)"></a>TTA(Test Time Augmentation)</h2><p>模型若僅進行一次預測的結果不一定能得到正確答案，針對測試資料做影像加強，包括不同區域裁剪和更改縮放程度等，並對不同版本的圖像進行預測，能得到多個結果，最後進行平均輸出作為最終結果，如此能夠提高結果的穩定性與精確度。在加入TTA後，LB分數有了顯著的上升</p><blockquote><p>基於EfficientNet b4的模型，使用TTA前後對比</p></blockquote><table><thead><tr><th align="center">Model</th><th align="center">bs&#x2F;ep&#x2F;sz</th><th align="center">post process</th><th align="center">LB</th></tr></thead><tbody><tr><td align="center"><strong>EfficientNet b4</strong></td><td align="center">64&#x2F;10&#x2F;224</td><td align="center">-</td><td align="center">0.805</td></tr><tr><td align="center"><strong>EfficientNet b4</strong></td><td align="center">64&#x2F;10&#x2F;224</td><td align="center">TTA</td><td align="center">0.813</td></tr></tbody></table><h2 id="Model-Ensemble"><a href="#Model-Ensemble" class="headerlink" title="Model Ensemble"></a>Model Ensemble</h2><p>訓練多組模型，將預測結果做平均，以降低模型的bias、variance，提升預測準確度及穩定度，進行多組實驗後我們決定選用EfficientNet b4、EfficientNet b5進行Ensemble。</p><blockquote><p> Model Ensemble前後對比</p></blockquote><table><thead><tr><th align="center">Model</th><th align="center">bs&#x2F;ep&#x2F;sz</th><th align="center">CV</th><th align="center">LB</th></tr></thead><tbody><tr><td align="center"><strong>EfficientNet b4</strong></td><td align="center">64&#x2F;10&#x2F;224</td><td align="center">0.9381</td><td align="center">0.813</td></tr><tr><td align="center"><strong>EfficientNet b5</strong></td><td align="center">32&#x2F;10&#x2F;260</td><td align="center">0.9454</td><td align="center">0.817</td></tr><tr><td align="center"><strong>Ensemble b4&amp;b5</strong></td><td align="center"></td><td align="center"></td><td align="center">0.822</td></tr></tbody></table><h2 id="後處理與預測流程圖"><a href="#後處理與預測流程圖" class="headerlink" title="後處理與預測流程圖"></a>後處理與預測流程圖</h2><p><img src="/2022/08/08/blindness-detect/%E5%BE%8C%E8%99%95%E7%90%86%E8%88%87%E9%A0%90%E6%B8%AC%E6%B5%81%E7%A8%8B%E5%9C%96.png" alt="後處理與預測流程圖"></p><h1 id="競賽成果"><a href="#競賽成果" class="headerlink" title="競賽成果"></a>競賽成果</h1><p>此競賽全球共2943隊參加，本隊獲得本次競賽銀牌第19名(Top1%)</p><table><thead><tr><th align="center">Leader board</th><th align="center">Rank</th><th align="center">Score</th></tr></thead><tbody><tr><td align="center"><strong>Public</strong></td><td align="center">88</td><td align="center">0.822879</td></tr><tr><td align="center"><strong>Private</strong></td><td align="center">19 (Final standings)</td><td align="center">0.929524</td></tr></tbody></table><p><img src="/2022/08/08/blindness-detect/%E7%AB%B6%E8%B3%BD%E6%88%90%E6%9E%9C.png" alt="競賽成果"></p><p>此作品參與2020元智大學人工智慧競賽技術與應用組，榮獲優等</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/blindness-detect/競賽成果2.jpg" alt="競賽成果2" style="zoom:33%;"></div></div></div><h1 id="GUI-設計"><a href="#GUI-設計" class="headerlink" title="GUI 設計"></a>GUI 設計</h1><p>在競賽過程中，設計出一套精確的模型，為了使模型達到最佳效益，我們著手設計使用者介面並結合模型，讓偵測糖尿病視網膜病變能夠具體化，且讓使用者在介面操作上能夠快速上手。</p><p><img src="/2022/08/08/blindness-detect/GUI2.png" alt="GUI2"></p>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
      <category>大學專題</category>
      
    </categories>
    
    
    <tags>
      
      <tag>影像處理</tag>
      
      <tag>人工智慧</tag>
      
      <tag>人機介面</tag>
      
      <tag>Kaggle</tag>
      
      <tag>大數據競賽</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>愛文芒果不良品辨識競賽</title>
    <link href="/2022/08/08/ai-mango/"/>
    <url>/2022/08/08/ai-mango/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品為課程專題並參與大數據競賽</p><p>此作品榮獲 2020 AI CUP 愛文芒果五類不良品辨識競賽前標</p></blockquote><h1 id="比賽簡介"><a href="#比賽簡介" class="headerlink" title="比賽簡介"></a>比賽簡介</h1><p>台灣重要出口農產品之一的愛文芒果於近年銷量持續增長，不僅躍升為三大外銷高經濟生鮮果品之一，更將外銷國拓展至日本、中國、美國以及香港等地。</p><p>本競賽針對愛文芒果進行五類不良品分類，包括乳汁吸附、機械傷害、炭疽病、著色不佳、黑斑病等，以影像辨識快速分辨愛文芒果不良品產生原因，期許在未來以此資料進行分析及預測，提供生產者資訊，降低愛文芒果劣果率。</p><h1 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h1><p>芒果圖像透過果農拍照病標註受傷部位所收集而來，每張照片芒果可能擁有多種受傷類別，故為多分類問題。</p><ul><li><p>25768 Training set </p><p>圖像、分類標籤、受傷位置</p></li><li><p>3681 Development set</p><p>圖像、分類標籤、受傷位置</p></li><li><p>Test set</p><p>圖像</p></li></ul><h2 id="資料集圖像示意圖"><a href="#資料集圖像示意圖" class="headerlink" title="資料集圖像示意圖"></a>資料集圖像示意圖</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/ai-mango/38414.jpg" alt="38414" style="zoom:25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/ai-mango/19992.jpg" alt="19992" style="zoom:25%;"></div></div></div><h2 id="芒果受傷種類共分為五類"><a href="#芒果受傷種類共分為五類" class="headerlink" title="芒果受傷種類共分為五類"></a>芒果受傷種類共分為五類</h2><p>下表病徵圖透過原始資料集取出，一張圖像可能擁有多種病癥。</p><table><thead><tr><th align="center">不良品類別</th><th align="center">芒果圖</th></tr></thead><tbody><tr><td align="center">機械傷害</td><td align="center"><img src="/2022/08/08/ai-mango/40462_0.jpg" alt="40462_0"></td></tr><tr><td align="center">炭疽病</td><td align="center"><img src="/2022/08/08/ai-mango/00019_0.jpg" alt="00019_0" style="zoom:50%;"></td></tr><tr><td align="center">黑斑病</td><td align="center"><img src="/2022/08/08/ai-mango/47863_0.jpg" alt="47863_0" style="zoom:150%;"></td></tr><tr><td align="center">著色不佳</td><td align="center"><img src="/2022/08/08/ai-mango/00792_0.jpg" alt="00792_0" style="zoom: 50%;"></td></tr><tr><td align="center">乳汁吸附</td><td align="center"><img src="/2022/08/08/ai-mango/02015_0.jpg" alt="02015_0"></td></tr></tbody></table><h2 id="資料分析"><a href="#資料分析" class="headerlink" title="資料分析"></a>資料分析</h2><p> 圖表發現只有單一種炭疽病或著色不佳此兩種病癥佔多數，其餘為多種病癥混合，或其他三種症狀，資料嚴重不平衡。  </p><p><img src="/2022/08/08/ai-mango/image-20220605152135404.png" alt="資料分析表"></p><h2 id="資料增強-Data-Augment"><a href="#資料增強-Data-Augment" class="headerlink" title="資料增強(Data Augment)"></a>資料增強(Data Augment)</h2><p>訓練資料集的25768張圖片中，有87%(22522張圖片)只有一種病徵，故於訓練階段時用這22522張圖片做訓練，期望可以更準確的抓取病徵特徵，而不會產生病徵之間的混淆。</p><p>針對機械傷害、黑斑病、乳汁吸附三類圖像進行資料擴增，進行圖片翻轉、平移、明暗度、比例尺等方式將此三種類別資料擴增。</p><h1 id="模型訓練"><a href="#模型訓練" class="headerlink" title="模型訓練"></a>模型訓練</h1><p>模型架構為EfficientNetB0 + Dence(128) + 五個輸出層(Dence(1)) ，並且不使用pretrain_weight，讓模型重新訓練，optimizer使用Adam，模型中加入sample_weight參數，針對有著色不佳的圖片sample_weight設為5，其他病徵則設為1，將模型設定為多輸出模型(對輸入的圖像作五個binary的預測)。</p><p>模型最後的輸出為五個值，所以設定每個類別各自的閥值。在本次競賽中我們將五個類別閥值皆設為0.15。若閥值設定太高，會有一張圖片五個預測值都為0的情況。<img src="/2022/08/08/ai-mango/image-20220605172834946.png" alt="網路架構"></p><h1 id="評估公式"><a href="#評估公式" class="headerlink" title="評估公式"></a>評估公式</h1><p><img src="/2022/08/08/ai-mango/image-20220605220303830.png" alt="評估標準"></p><h1 id="競賽成果"><a href="#競賽成果" class="headerlink" title="競賽成果"></a>競賽成果</h1><p>將預測結果存成特定csv格式，上傳結果即可獲得排名與分數。</p><ul><li>Public 為競賽期間測試Development set最佳成果</li><li>Private為測試Test set成果</li></ul><table><thead><tr><th>Leaderboard</th><th>Rank</th><th>Score</th></tr></thead><tbody><tr><td>Public</td><td>37&#x2F;222</td><td>0.6573533</td></tr><tr><td>Private</td><td>40&#x2F;222</td><td>0.6628388</td></tr></tbody></table><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/08/08/ai-mango/競賽成果.jpg" alt="競賽成果" style="zoom:33%;"></div></div></div>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI CUP 2020</tag>
      
      <tag>影像處理</tag>
      
      <tag>人工智慧</tag>
      
      <tag>大數據競賽</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>結合虛擬實境與慣性姿態感測器之防疫機器代理人</title>
    <link href="/2022/06/03/future-tech/"/>
    <url>/2022/06/03/future-tech/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品<strong>2021未來科技獎競賽獲獎</strong></p></blockquote><h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><p>使用者將會透過VR眼鏡看到機器人360環景影像，接著使用者姿態動作改變時，其姿態動作感測訊號將透過WIFI送至雲端進行深度學習模型辨識。當模型辨識到使用者之動作時，將驅動機器人進行相同的動作。</p><!-- more --><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><ol><li>受試者穿戴VR裝置，並安裝360鏡頭於機器人身上，使受試者與機器人有相同的視野，能夠顯得更加臨場感。</li><li>姿態感測器組成一套人體姿態量測系統，利用演算法進行角度融合運算，透過Wi-Fi將資料傳輸到電腦端進行所有感測器的整合與姿態計算。</li><li>以CNN+LSTM模型訓練九軸資料，並於測試端進行即時動作識別，再將所判斷的動作指令透過藍芽傳輸與機器人作溝通，命令機器人作出相對應的動作。</li></ol><p><img src="/2022/06/03/future-tech/image-20220603234742622.png" alt="系統架構圖"></p><h2 id="使用設備"><a href="#使用設備" class="headerlink" title="使用設備"></a>使用設備</h2><h3 id="設備總覽"><a href="#設備總覽" class="headerlink" title="設備總覽"></a>設備總覽</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/image-20220604031854938.png" alt="使用設備" style="zoom: 50%;"></div></div></div><h3 id="慣性感測器模組"><a href="#慣性感測器模組" class="headerlink" title="慣性感測器模組"></a>慣性感測器模組</h3><p>自行開發的慣性感測器，慣性感測單元具有九軸加速規的高感度設計，內含三軸陀螺儀、加速度計及磁力計，角度探測精度可以小於0.1度。</p><p>Wifi傳輸模組，可以輕鬆地將串列設備連接至網路，透過UART與微控制器溝通。微處理器用來控制周遭的電路。</p><img src="/2022/06/03/future-tech/image-20220604032002448.png" alt="慣性感測器硬體系統" style="zoom: 67%;"><h3 id="VR-amp-360相機"><a href="#VR-amp-360相機" class="headerlink" title="VR&amp;360相機"></a>VR&amp;360相機</h3><p>360相機為theta v，擁有360度全景直播功能，透過直播網路串流將影像投射到Unity球體上，再將VR視角置於球體正中心，戴上VR眼鏡後，即可如身歷其境般看到機器人的360視角</p><img src="/2022/06/03/future-tech/image-20220604033232448.png" alt="VR&360相機" style="zoom:67%;"><h2 id="模型架構"><a href="#模型架構" class="headerlink" title="模型架構"></a>模型架構</h2><p>因考慮到機器人控制之即時性，姿態感測訊號訓練資料時間範圍為動作開始時間點後0.1秒，訓練Deep Conv-LSTM 模型，進行動作姿態辨識。</p><p>受試者皆可以有效完成六種指定之動作，包括左手抬起、右手抬起、雙手抬起、前進、左轉以及右轉之動作，並且可在各種環境下皆可進行使用，不易受到外界雜訊干擾。</p><img src="/2022/06/03/future-tech/image-20220604030119199.png" alt="AI模型架構" style="zoom: 50%;"><h2 id="動作識別"><a href="#動作識別" class="headerlink" title="動作識別"></a>動作識別</h2><p>使用者執行動作時，姿態慣性感測訊號送至深度學習模型進行判別，並根據其判別結果，送出動作指令給機器人，使機器人做出相對應之動作。</p><p><img src="/2022/06/03/future-tech/image-20220604030503219.png"></p><h1 id="實體展示"><a href="#實體展示" class="headerlink" title="實體展示"></a>實體展示</h1><p>於台北世貿實體展示，並開放觀眾參觀與詢問問題。</p><p>向高中學生展示與介紹我們的系統，開放參觀者親自體驗我們的系統</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7708.png" alt="IMG_7708" style="zoom: 25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7704.png" alt="IMG_7704" style="zoom: 25%;"></div></div></div><p><strong>此作品榮獲未來科技獎</strong></p><p><img src="/2022/06/03/future-tech/IMG_7743.png"></p><p><img src="/2022/06/03/future-tech/IMG_7802.png"></p><blockquote><p>完整競賽影片 : <a href="https://youtu.be/XFN2XDS2GfY"><em>未來科技獎</em></a> (於影片後段有demo影片)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>機器人</tag>
      
      <tag>2021未來科技獎</tag>
      
      <tag>人工智慧</tag>
      
      <tag>系統設計</tag>
      
      <tag>虛擬實境</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>運動捕捉系統</title>
    <link href="/2022/06/02/imu_skeleton/"/>
    <url>/2022/06/02/imu_skeleton/</url>
    
    <content type="html"><![CDATA[<h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><blockquote><p>碩士研究 Project</p></blockquote><p>​配置感測器於操作者之胸部、臀部、左右手之手腕、手軸、手臂、肩膀、左右腳之大腿與小腿，而感測器配戴方式為將每個感測器之燈號方向朝向軀幹，以軀幹為中心所配置。配戴感測器後，同時偵測全身感測器之四元數，透過藍芽傳輸四元數數據至電腦端，並將IMU依據關節分段儲存（IMU to segment, I2S），進行I2S分配與I2S矯正後即可透過四元數控制相對應之關節點旋轉，於Unity遊戲引擎內呈現出人體姿態。</p><!-- more --><h1 id="系統特色"><a href="#系統特色" class="headerlink" title="系統特色"></a>系統特色</h1><ul><li><p>Record mode</p><p>​存取動畫骨骼中25個關節之三軸座標位置與對應時間點，透過錄製鍵與存取鍵將人體姿態依據csv格式做儲存。(於程式中可再加入其他部位的關節資訊與關節旋轉角)</p></li><li><p>Real-time mode</p><p>​將動畫中25個關節位置資訊實時存取，並以txt格式存取並實時更新關節旋轉角與位置資訊。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/關節位置.png" style="zoom: 80%;"></div></div></div></li></ul><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><p><img src="/2022/06/02/imu_skeleton/%E9%81%8B%E5%8B%95%E6%8D%95%E6%8D%89%E7%B3%BB%E7%B5%B1.png"></p><h2 id="I2S-矯正"><a href="#I2S-矯正" class="headerlink" title="I2S 矯正"></a>I2S 矯正</h2><p>慣性感測器(IMU)與動畫人體模型各自擁有專屬的座標系，圖中紅色為X軸、藍色為Y軸、綠色為Z軸，IMU穿戴於人體身上後無法與Unity座標系相符合，需透過四元數運算將全身慣性感測器座標系轉換至Unity座標系，進行I2S矯正並紀錄正確四元數數值，以達成人體姿態重定向目標。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/坐標系.png" style="zoom: 67%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/%E7%9F%AF%E6%AD%A3.png"></div></div></div><p>​使用者穿戴慣性感測器時將以T-pose進行姿態矯正，下面影片中動畫呈現奇怪姿態，因IMU座標軸尚未與Unity座標軸對齊，操作者須保持T-pose，按下calibrate鍵，即矯正完畢，可呈現出人體正確姿態。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/image-20220602180733685.png" alt="image-20220602180733685" style="zoom: 25%;"></div></div></div><p><img src="/2022/06/02/imu_skeleton/calibrate.gif"></p><h1 id="系統功能呈現"><a href="#系統功能呈現" class="headerlink" title="系統功能呈現"></a>系統功能呈現</h1><h2 id="運動捕捉系統"><a href="#運動捕捉系統" class="headerlink" title="運動捕捉系統"></a>運動捕捉系統</h2><p>運動捕捉系統即時動畫呈現</p><p><img src="/2022/06/02/imu_skeleton/forward.gif"></p><p><img src="/2022/06/02/imu_skeleton/rightward.gif"></p><p><img src="/2022/06/02/imu_skeleton/backward.gif"></p><p><img src="/2022/06/02/imu_skeleton/leftward_and_sit.gif"></p><blockquote><p>完整demo 影片: <a href="https://www.youtube.com/watch?v=ozX-XhBwfu8"><em>IMU in Unity Skeleton</em></a></p></blockquote><h2 id="可示化人體骨骼"><a href="#可示化人體骨骼" class="headerlink" title="可示化人體骨骼"></a>可示化人體骨骼</h2><p>錄製關節角度後於python端呈現人體骨骼姿態</p><p><img src="/2022/06/02/imu_skeleton/handwave2.gif" alt="skeleton"></p>]]></content>
    
    
    <categories>
      
      <category>碩士研究</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>運動捕捉</tag>
      
      <tag>運動重定向</tag>
      
      <tag>人機介面</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
