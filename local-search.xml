<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>愛文芒果不良品分類競賽</title>
    <link href="/2022/06/04/ai-mango/"/>
    <url>/2022/06/04/ai-mango/</url>
    
    <content type="html"><![CDATA[<h1 id="比賽簡介"><a href="#比賽簡介" class="headerlink" title="比賽簡介"></a>比賽簡介</h1><p>台灣重要出口農產品之一的愛文芒果於近年銷量持續增長，不僅躍升為三大外銷高經濟生鮮果品之一，更將外銷國拓展至日本、中國、美國以及香港等地。</p><p>本競賽針對愛文芒果進行五類不良品分類，包括乳汁吸附、機械傷害、炭疽病、著色不佳、黑斑病等，以影像辨識快速分辨愛文芒果不良品產生原因，期許在未來以此資料進行分析及預測，提供生產者資訊，降低愛文芒果劣果率。</p><h1 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h1><p>芒果圖像透過果農拍照病標註受傷部位所收集而來，每張照片芒果可能擁有多種受傷類別，故為多分類問題。</p><ul><li><p>25768 Training set </p><p>圖像、分類標籤、受傷位置</p></li><li><p>3681 Development set</p><p>圖像、分類標籤、受傷位置</p></li><li><p>Test set</p><p>圖像</p></li></ul><h2 id="資料集圖像示意圖"><a href="#資料集圖像示意圖" class="headerlink" title="資料集圖像示意圖"></a>資料集圖像示意圖</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/04/ai-mango/38414.jpg" alt="38414" style="zoom:25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/04/ai-mango/19992.jpg" alt="19992" style="zoom:25%;"></div></div></div><h2 id="芒果受傷種類共分為五類"><a href="#芒果受傷種類共分為五類" class="headerlink" title="芒果受傷種類共分為五類"></a>芒果受傷種類共分為五類</h2><p>下表病徵圖透過原始資料集取出，一張圖像可能擁有多種病癥。</p><table><thead><tr><th align="center">不良品類別</th><th align="center">芒果圖</th></tr></thead><tbody><tr><td align="center">機械傷害</td><td align="center"><img src="/2022/06/04/ai-mango/40462_0.jpg" alt="40462_0"></td></tr><tr><td align="center">炭疽病</td><td align="center"><img src="/2022/06/04/ai-mango/00019_0.jpg" alt="00019_0" style="zoom:50%;"></td></tr><tr><td align="center">黑斑病</td><td align="center"><img src="/2022/06/04/ai-mango/47863_0.jpg" alt="47863_0" style="zoom:150%;"></td></tr><tr><td align="center">著色不佳</td><td align="center"><img src="/2022/06/04/ai-mango/00792_0.jpg" alt="00792_0" style="zoom: 50%;"></td></tr><tr><td align="center">乳汁吸附</td><td align="center"><img src="/2022/06/04/ai-mango/02015_0.jpg" alt="02015_0"></td></tr></tbody></table><h2 id="資料分析"><a href="#資料分析" class="headerlink" title="資料分析"></a>資料分析</h2><p> 圖表發現只有單一種炭疽病或著色不佳此兩種病癥佔多數，其餘為多種病癥混合，或其他三種症狀，資料嚴重不平衡。  </p><p><img src="/2022/06/04/ai-mango/image-20220605152135404.png" alt="資料分析表"></p><h2 id="資料增強-Data-Augment"><a href="#資料增強-Data-Augment" class="headerlink" title="資料增強(Data Augment)"></a>資料增強(Data Augment)</h2><p>訓練資料集的25768張圖片中，有87%(22522張圖片)只有一種病徵，故於訓練階段時用這22522張圖片做訓練，期望可以更準確的抓取病徵特徵，而不會產生病徵之間的混淆。</p><p>針對機械傷害、黑斑病、乳汁吸附三類圖像進行資料擴增，進行圖片翻轉、平移、明暗度、比例尺等方式將此三種類別資料擴增。</p><h1 id="模型訓練"><a href="#模型訓練" class="headerlink" title="模型訓練"></a>模型訓練</h1><p>模型架構為EfficientNetB0 + Dence(128) + 五個輸出層(Dence(1)) ，並且不使用pretrain_weight，讓模型重新訓練，optimizer使用Adam，模型中加入sample_weight參數，針對有著色不佳的圖片sample_weight設為5，其他病徵則設為1，將模型設定為多輸出模型(對輸入的圖像作五個binary的預測)。</p><p>模型最後的輸出為五個值，所以設定每個類別各自的閥值。在本次競賽中我們將五個類別閥值皆設為0.15。若閥值設定太高，會有一張圖片五個預測值都為0的情況。<img src="/2022/06/04/ai-mango/image-20220605172834946.png" alt="網路架構"></p><h1 id="評估公式"><a href="#評估公式" class="headerlink" title="評估公式"></a>評估公式</h1><p><img src="/2022/06/04/ai-mango/image-20220605220303830.png" alt="評估標準"></p><h1 id="競賽成果"><a href="#競賽成果" class="headerlink" title="競賽成果"></a>競賽成果</h1><p>將預測結果存成特定csv格式，上傳結果即可獲得排名與分數。</p><ul><li>Public 為競賽期間測試Development set最佳成果</li><li>Private為測試Test set成果</li></ul><table><thead><tr><th>Leaderboard</th><th>Rank</th><th>Score</th></tr></thead><tbody><tr><td>Public</td><td>37&#x2F;222</td><td>0.6573533</td></tr><tr><td>Private</td><td>40&#x2F;222</td><td>0.6628388</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI CUP 2020</tag>
      
      <tag>影像處理</tag>
      
      <tag>人工智慧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>結合虛擬實境與慣性姿態感測器之防疫機器代理人</title>
    <link href="/2022/06/03/future-tech/"/>
    <url>/2022/06/03/future-tech/</url>
    
    <content type="html"><![CDATA[<blockquote><p>此作品<strong>2021未來科技獎競賽獲獎</strong></p></blockquote><h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><p>使用者將會透過VR眼鏡看到機器人360環景影像，接著使用者姿態動作改變時，其姿態動作感測訊號將透過WIFI送至雲端進行深度學習模型辨識。當模型辨識到使用者之動作時，將驅動機器人進行相同的動作。</p><!-- more --><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><ol><li>受試者穿戴VR裝置，並安裝360鏡頭於機器人身上，使受試者與機器人有相同的視野，能夠顯得更加臨場感。</li><li>姿態感測器組成一套人體姿態量測系統，利用演算法進行角度融合運算，透過Wi-Fi將資料傳輸到電腦端進行所有感測器的整合與姿態計算。</li><li>以CNN+LSTM模型訓練九軸資料，並於測試端進行即時動作識別，再將所判斷的動作指令透過藍芽傳輸與機器人作溝通，命令機器人作出相對應的動作。</li></ol><p><img src="/2022/06/03/future-tech/image-20220603234742622.png" alt="系統架構圖"></p><h2 id="使用設備"><a href="#使用設備" class="headerlink" title="使用設備"></a>使用設備</h2><h3 id="設備總覽"><a href="#設備總覽" class="headerlink" title="設備總覽"></a>設備總覽</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/image-20220604031854938.png" alt="使用設備" style="zoom: 50%;"></div></div></div><h3 id="慣性感測器模組"><a href="#慣性感測器模組" class="headerlink" title="慣性感測器模組"></a>慣性感測器模組</h3><p>自行開發的慣性感測器，慣性感測單元具有九軸加速規的高感度設計，內含三軸陀螺儀、加速度計及磁力計，角度探測精度可以小於0.1度。</p><p>Wifi傳輸模組，可以輕鬆地將串列設備連接至網路，透過UART與微控制器溝通。微處理器用來控制周遭的電路。</p><img src="/2022/06/03/future-tech/image-20220604032002448.png" alt="慣性感測器硬體系統" style="zoom: 67%;"><h3 id="VR-amp-360相機"><a href="#VR-amp-360相機" class="headerlink" title="VR&amp;360相機"></a>VR&amp;360相機</h3><p>360相機為theta v，擁有360度全景直播功能，透過直播網路串流將影像投射到Unity球體上，再將VR視角置於球體正中心，戴上VR眼鏡後，即可如身歷其境般看到機器人的360視角</p><img src="/2022/06/03/future-tech/image-20220604033232448.png" alt="VR&360相機" style="zoom:67%;"><h2 id="模型架構"><a href="#模型架構" class="headerlink" title="模型架構"></a>模型架構</h2><p>因考慮到機器人控制之即時性，姿態感測訊號訓練資料時間範圍為動作開始時間點後0.1秒，訓練Deep Conv-LSTM 模型，進行動作姿態辨識。</p><p>受試者皆可以有效完成六種指定之動作，包括左手抬起、右手抬起、雙手抬起、前進、左轉以及右轉之動作，並且可在各種環境下皆可進行使用，不易受到外界雜訊干擾。</p><img src="/2022/06/03/future-tech/image-20220604030119199.png" alt="AI模型架構" style="zoom: 50%;"><h2 id="動作識別"><a href="#動作識別" class="headerlink" title="動作識別"></a>動作識別</h2><p>使用者執行動作時，姿態慣性感測訊號送至深度學習模型進行判別，並根據其判別結果，送出動作指令給機器人，使機器人做出相對應之動作。</p><p><img src="/2022/06/03/future-tech/image-20220604030503219.png"></p><h1 id="實體展示"><a href="#實體展示" class="headerlink" title="實體展示"></a>實體展示</h1><p>於台北世貿實體展示，並開放觀眾參觀與詢問問題。</p><p>向高中學生展示與介紹我們的系統，開放參觀者親自體驗我們的系統</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7708.png" alt="IMG_7708" style="zoom: 25%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/03/future-tech/IMG_7704.png" alt="IMG_7704" style="zoom: 25%;"></div></div></div><p><strong>此作品榮獲未來科技獎</strong></p><p><img src="/2022/06/03/future-tech/IMG_7743.png"></p><p><img src="/2022/06/03/future-tech/IMG_7802.png"></p><blockquote><p>完整競賽影片 : <a href="https://youtu.be/XFN2XDS2GfY"><em>未來科技獎</em></a> (於影片後段有demo影片)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>競賽</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>機器人</tag>
      
      <tag>2021未來科技獎</tag>
      
      <tag>人工智慧</tag>
      
      <tag>系統設計</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>運動捕捉系統</title>
    <link href="/2022/06/02/imu_skeleton/"/>
    <url>/2022/06/02/imu_skeleton/</url>
    
    <content type="html"><![CDATA[<h1 id="系統簡介"><a href="#系統簡介" class="headerlink" title="系統簡介"></a>系統簡介</h1><blockquote><p>碩士研究 Project</p></blockquote><p>​配置感測器於操作者之胸部、臀部、左右手之手腕、手軸、手臂、肩膀、左右腳之大腿與小腿，而感測器配戴方式為將每個感測器之燈號方向朝向軀幹，以軀幹為中心所配置。配戴感測器後，同時偵測全身感測器之四元數，透過藍芽傳輸四元數數據至電腦端，並將IMU依據關節分段儲存（IMU to segment, I2S），進行I2S分配與I2S矯正後即可透過四元數控制相對應之關節點旋轉，於Unity遊戲引擎內呈現出人體姿態。</p><!-- more --><h1 id="系統特色"><a href="#系統特色" class="headerlink" title="系統特色"></a>系統特色</h1><ul><li><p>Record mode</p><p>​存取動畫骨骼中25個關節之三軸座標位置與對應時間點，透過錄製鍵與存取鍵將人體姿態依據csv格式做儲存。(於程式中可再加入其他部位的關節資訊與關節旋轉角)</p></li><li><p>Real-time mode</p><p>​將動畫中25個關節位置資訊實時存取，並以txt格式存取並實時更新關節旋轉角與位置資訊。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/關節位置.png" style="zoom: 80%;"></div></div></div></li></ul><h1 id="系統架構"><a href="#系統架構" class="headerlink" title="系統架構"></a>系統架構</h1><p><img src="/2022/06/02/imu_skeleton/%E9%81%8B%E5%8B%95%E6%8D%95%E6%8D%89%E7%B3%BB%E7%B5%B1.png"></p><h2 id="I2S-矯正"><a href="#I2S-矯正" class="headerlink" title="I2S 矯正"></a>I2S 矯正</h2><p>慣性感測器(IMU)與動畫人體模型各自擁有專屬的座標系，圖中紅色為X軸、藍色為Y軸、綠色為Z軸，IMU穿戴於人體身上後無法與Unity座標系相符合，需透過四元數運算將全身慣性感測器座標系轉換至Unity座標系，進行I2S矯正並紀錄正確四元數數值，以達成人體姿態重定向目標。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/坐標系.png" style="zoom: 67%;"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/%E7%9F%AF%E6%AD%A3.png"></div></div></div><p>​使用者穿戴慣性感測器時將以T-pose進行姿態矯正，下面影片中j動畫呈現奇怪姿態，因IMU座標軸尚未與Unity座標軸對齊，操作者須保持T-pose，按下calibrate鍵，即矯正完畢，可呈現出人體正確姿態。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2022/06/02/imu_skeleton/image-20220602180733685.png" alt="image-20220602180733685" style="zoom: 25%;"></div></div></div><p><img src="/2022/06/02/imu_skeleton/calibrate.gif"></p><h1 id="系統功能呈現"><a href="#系統功能呈現" class="headerlink" title="系統功能呈現"></a>系統功能呈現</h1><h2 id="運動捕捉系統"><a href="#運動捕捉系統" class="headerlink" title="運動捕捉系統"></a>運動捕捉系統</h2><p>運動捕捉系統即時動畫呈現</p><p><img src="/2022/06/02/imu_skeleton/forward.gif"></p><p><img src="/2022/06/02/imu_skeleton/rightward.gif"></p><p><img src="/2022/06/02/imu_skeleton/backward.gif"></p><p><img src="/2022/06/02/imu_skeleton/leftward_and_sit.gif"></p><blockquote><p>完整demo 影片: <a href="https://www.youtube.com/watch?v=ozX-XhBwfu8"><em>IMU in Unity Skeleton</em></a></p></blockquote><h2 id="可示化人體骨骼"><a href="#可示化人體骨骼" class="headerlink" title="可示化人體骨骼"></a>可示化人體骨骼</h2><p>錄製關節角度後於python端呈現人體骨骼姿態</p><p><img src="/2022/06/02/imu_skeleton/handwave2.gif" alt="skeleton"></p>]]></content>
    
    
    <categories>
      
      <category>碩士研究</category>
      
    </categories>
    
    
    <tags>
      
      <tag>慣性感測器</tag>
      
      <tag>運動捕捉</tag>
      
      <tag>運動重定向</tag>
      
      <tag>人機介面</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
